{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs = pd.read_csv(\"doj_songs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Data\n",
    "\n",
    "# merge all characters into one string\n",
    "text = \"\"\n",
    "clean = \"\"\n",
    "for line in songs[\"lyrics\"]:\n",
    "    text = text + str(line).lower()\n",
    "    clean = clean + \" \".join(re.findall(r\"[a-z']+\", text))\n",
    "\n",
    "# find all unique characters\n",
    "tokens = re.findall(r\"[a-z'\\s]\", clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Alphabet\n",
    "\n",
    "characters = sorted(list(set(tokens)))\n",
    "len(characters)\n",
    "# 28 unique characters\n",
    "\n",
    "# dictionary for character-to-index mapping\n",
    "char_to_index = dict((char, index) for index, char in enumerate(characters))\n",
    "\n",
    "# dictionary for index-to-character mapping\n",
    "index_to_char = dict((index, char) for index, char in enumerate(characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Training Sequences\n",
    "\n",
    "# chunk the text into sequences\n",
    "maxlen = 20  # n\n",
    "step = 1  # length of step at each iteration\n",
    "\n",
    "# list of sequences\n",
    "sequences = []\n",
    "\n",
    "# list of next characters model should predict\n",
    "next_characters = []\n",
    "\n",
    "# iterate over cleaned text string and each 20-length sequence into list\n",
    "for i in range(0, len(clean) - maxlen, step):\n",
    "    sequences.append(clean[i : (i + maxlen)])\n",
    "    next_characters.append(clean[i + maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jl/scpv3h4s1gg9bhlvnrq23jxm0000gn/T/ipykernel_96839/962578967.py:10: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  x = np.zeros((len(sequences), maxlen, len(characters)), dtype=np.bool)  # input\n",
      "/var/folders/jl/scpv3h4s1gg9bhlvnrq23jxm0000gn/T/ipykernel_96839/962578967.py:11: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  y = np.zeros((len(sequences), len(characters)), dtype=np.bool)  # output\n"
     ]
    }
   ],
   "source": [
    "# Label Encode Training Sequences (one-hot encoding)\n",
    "\n",
    "# create empty matrices for input and output sets\n",
    "# input: each n-length sequence in sequences list\n",
    "# output: next character after each n-length sequence\n",
    "# i.e.: sentence = \"hello there\"\n",
    "#       sequence = \"hel\"\n",
    "#       next char = \"l\"\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(characters)), dtype=np.bool)  # input\n",
    "y = np.zeros((len(sequences), len(characters)), dtype=np.bool)  # output\n",
    "\n",
    "for i, chunk in enumerate(sequences):\n",
    "    for j, c in enumerate(chunk):\n",
    "        x[i, j, char_to_index[c]] = 1\n",
    "    y[i, char_to_index[next_characters[i]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model\n",
    "A single LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-11 00:40:00.544635: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(maxlen, len(characters))),\n",
    "        layers.LSTM(128),\n",
    "        layers.Dense(len(characters), activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 128)               80384     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 28)                3612      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83,996\n",
      "Trainable params: 83,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Text Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample an index from a probability array\n",
    "\n",
    "def sample(predictions, temperature=1.0):\n",
    "    predictions = np.asarray(predictions).astype(\"float64\")\n",
    "    predictions = np.log(predictions) / temperature\n",
    "    exp_predictions = np.exp(predictions)\n",
    "    predictions = exp_predictions / np.sum(exp_predictions)\n",
    "    probabilities = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  603/19275 [..............................] - ETA: 22:32 - loss: 1.2906"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    model.fit(x, y, batch_size = size, epochs=1)\n",
    "    print()\n",
    "    print(\"Generating text after epoch: %d\" %epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print(\"Diversity: \", diversity)\n",
    "\n",
    "        generated = \"\"\n",
    "        sentence = clean[start_index:start_index + maxlen]\n",
    "        print(\"Generating with seed: '\" + sentence + \"'\")\n",
    "\n",
    "        for i in range(400):\n",
    "            x_predict = np.zeros((1, maxlen, len(characters)))\n",
    "            \n",
    "            for t, char in enumerate(sentence):\n",
    "                x_predict[0, t, char_to_index[char]] = 1.0\n",
    "            predictions = model.predict(x_predict, verbose = 0)[0]\n",
    "            next_index = sample(predictions, diversity)\n",
    "            next_char = index_to_char[next_index]\n",
    "            sentence = sentence[1:] + next_char\n",
    "            generated += next_char\n",
    "\n",
    "        print(\"Generated: \", generated)\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
